
i started my career in the technology when i was the first saw smartphone that was the first time when i was fascinated about the technology.

You know, when I was in 10th or maybe 11th grade, I often wondered why there weren't any films focused on core subjects like physics or history. I wished for a film that could teach each chapter as an episode, especially in physics. How amazing would that be?

Everyone would be excited to watch it, and no one would have to put in much effort to learn.

When I look back, I often wonder why those things seemed impossible in the past.

Some of you might say that creating a film requires a lot of money and manpower, right?

However, that has changed in 2024. Now, a person with a story and the determination to make it happen can produce a film alone, and the cost is a fraction—up to 1000 times cheaper—than before.

We are at the forefront of the AI revolution, reimagining the film and media industry.

we are very proud to say that we are not getting left behind.

Every week we experiment with new models, and new workflows and get our hands dirty and the reason is simple we are seeing the vision, a vision that will shape the future generation.

We launched our first film in the last week of July 2024.

Over the next 30 days, we dedicated 7-8 hours daily to this project. During that time, we began learning about diffusion models, and a new open-source model called Flux was released, which we experimented with as well. We learned how to train voices and maintain consistency in the images. We also started using ComfyUI and began writing my story script. After a month of hard work, we successfully uploaded the film on YT. Although it did not look like a film, I was satisfied with that because I got a taste of the future of the media industry.

It was all just experimentation.

Similarly, let me share another story. I come from a very underprivileged, tier-3 village, where advanced technology like smartphones was rare. The first time I saw a smartphone interacting with a human, I was amazed (I didn’t realize it was just Google Assistant). The way I felt at that moment is one of the reasons why I am working on these types of projects.

I’ve been involved with large language models (LLMs) for quite a while, and it’s all about interacting with users through conversation.

I’m grateful that I’ve been able to experiment with this technology, which has fascinated me since my childhood.

When AI was emerging, I started from reading research papers to building apps with ai, i went all the way.

As large language models (LLMs) improved, new features were introduced, including function calling. This means we can invoke external APIs or data within the LLM's input or output as JSON. While this may seem straightforward, it marks the beginning of a new era—an era of autonomy means a system that can take action on behalf of itself.

for memory for LLM applications, Retrieval-Augmented Generation (RAG), also known as semantic search, involves searching not by if-else logic but by using context.

